Missing values train.csv: 


****** Solo - 1647  ******

Country                                 73  Solo
Value at beginning of 2020/21 season    74  ---> 46 ( Only this value missing) + 
Value at beginning of 2021/22 season    30  ---> 3  ( Only this value missing )
Value at beginning of 2022/23 season     5  ---> 1  ( Only this value missing )
dtype: int64

Missing values test.csv: 

Country                                 17  All Solo 
Missing Country ids - 408, 1643, 892, 453, 764, 1696, 548, 67, 1681, 772, 170, 184, 1518, 593, 522, 309, 1456

Value at beginning of 2020/2021 season    25  - 16 ( Only this value missing ) 
Solo Missing 2020/21 ids - 1292, 438, 567, 1225, 1085, 426, 613, 237, 650, 240, 1196, 1494, 1648, 493, 620, 411

Value at beginning of 2021/21 season    10  - 1 solo missing , remaining all missed along with 2020/21
With 2020/2021 - 765, 1649, 49, 618, 694, 844, 679, 506, 420


****** Solo - 1647  ******

Value at beginning of 2022/23 season     0
dtype: int64


Number of unique countries: 73


* For the missing values in both train and test set , If country data is missing, No other data is missing, If other data is missing, Country is not missing , look at results/missing_data for more details

If Ridge and Lasso regression are performing well, but you still want to further improve their accuracy, there are several strategies and techniques you can consider:

1. **Feature Engineering:** Explore different ways to engineer your features or create new features. Feature engineering can sometimes reveal hidden patterns in the data that can lead to better model performance.

2. **Interaction Terms:** Experiment with adding interaction terms between features. These are combinations of two or more features that can capture non-linear relationships that linear models like Ridge and Lasso might miss.

3. **Polynomial Features:** You can try adding polynomial features to capture non-linear relationships. For example, you can include squared or cubed versions of your existing features.

4. **Outlier Detection and Handling:** Identify and handle outliers in your dataset, as outliers can sometimes negatively impact linear models. You can choose to remove outliers or transform them using robust techniques.

5. **Data Preprocessing:** Ensure that your data preprocessing steps are appropriate. Scaling and normalizing features can have a significant impact on the performance of linear models. You can also explore different encoding techniques for categorical variables.

6. **Regularization Strength:** Experiment with different values of the regularization strength (alpha) for Ridge and Lasso. Sometimes, fine-tuning this hyperparameter can lead to better results.

7. **Cross-Validation:** Perform thorough cross-validation to assess the model's generalization performance. This can help you detect overfitting and make necessary adjustments to the model or the data.

8. **Ensemble Models:** While Ridge and Lasso are linear models, you can explore the possibility of creating ensemble models. For example, you can combine multiple Ridge or Lasso models to improve predictive performance.

9. **Advanced Regression Techniques:** Consider trying more advanced regression techniques such as Elastic Net regression, which combines Ridge and Lasso regularization, or Bayesian Regression, which can provide robust estimates.

10. **Collect More Data:** If possible, collect more data to increase the size of your dataset. This can help linear models, especially when dealing with complex relationships.

11. **Check Assumptions:** Ensure that your data meets the assumptions of linear regression, such as linearity, independence of errors, and homoscedasticity. If these assumptions are violated, it might be necessary to explore other modeling approaches.

12. **Regularization Path:** Visualize the regularization path for Ridge and Lasso to understand how the coefficients change with different regularization strengths. This can provide insights into which features are important and which are not.

13. **Domain Knowledge:** Utilize domain knowledge to engineer features or guide the modeling process. Understanding the problem domain can often lead to better feature selection and model choices.

More Models: Consider adding more diverse models to your ensemble. For example, you could include gradient boosting machines (e.g., XGBoost, LightGBM) or other regression algorithms (e.g., SVR, k-Nearest Neighbors) to see if they contribute to better performance.

Data Augmentation: If you have a limited amount of data, consider data augmentation techniques to create additional training samples.

Ensemble Size: Experiment with the number of models in your ensemble. Sometimes, having more models can improve accuracy up to a point, but too many models can lead to diminishing returns or overfitting.

Ensemble Weighting: Instead of equal weights for all models, you can experiment with different weightings for each model in your ensemble to give more importance to the better-performing models.

Remember that there is no one-size-fits-all solution, and the effectiveness of these strategies may vary depending on the specific characteristics of your dataset. It's a good practice to experiment with different approaches and evaluate their impact on model performance through careful testing and cross-validation.


Main Dataset : 

Onehot - 50
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.44228057733666

Onehot - 20
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.437382168723714

Onehot - 50
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.442915793941314

Onehot - 20
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.440064584970193

Onehot - 50
Best Alpha: 1
Cross-validated Ridge RMSE: 10.442767062556133

Onehot - 30 - Brazil, Netherlands, Portugal, Argentina
Best Alpha: 1
Cross-validated Ridge RMSE: 10.429519480509407

Onehot - 20
Best Alpha: 1
Cross-validated Ridge RMSE: 10.433688439140482


Onehot - 50
Cross-validated Bayesian Ridge RMSE: 10.602706735256307

Onehot + freq - 50
Cross-validated Bayesian Ridge RMSE: 10.608021117895245


**** Polynomial features doesnt help 


## New full data : 

onehot20 -    
    # 'Country_Portugal',
    # 'Country_Netherlands',  
    
    # 'Country_Denmark',
    # 'Country_Belgium',
    # 'Country_Croatia',   

(env) λ python lasso_X.py
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.436374164273804

C:\Users\USER\Desktop\COMSYS\transfer-value-prediction\testing (main -> origin)
(env) λ python ridge_X.py
Best Alpha: 1
Cross-validated Ridge RMSE: 10.43251384360481

C:\Users\USER\Desktop\COMSYS\transfer-value-prediction\testing (main -> origin)
(env) λ python elasticnet.py
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.433146301651211
Cross-validated Bayesian Ridge RMSE: 10.593539532587851


(env) λ python Xgboost.pyv
Best Hyperparameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}
Cross-validated XGBoost RMSE: 11.04299981056809

λ python decisiontrees.py
Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}
Cross-validated Random Forest RMSE: 11.728882954683014


2020na : 

Onehot - 38
λ python ridge_X.py
Best Alpha: 1
Cross-validated Ridge RMSE: 10.714369928979997

Onehot - 38
λ python lasso_X.py
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.710971598936657

Onehot - 38
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.710651182439593
Cross-validated Bayesian Ridge RMSE: 10.873431822343045



2021na : 

Onehot - 20
λ python ridge_X.py
Best Alpha: 1
Cross-validated Ridge RMSE: 10.799316370718703

Onehot - 38
λ python ridge_X.py
Best Alpha: 1
Cross-validated Ridge RMSE: 10.800976493826433

Onehot - 20
λ python lasso_X.py
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.79650671779503

Onehot - 38
λ python lasso_X.py
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.795773246543988

Onehot - 20
λ python elasticnet.py
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.79685683397558
Cross-validated Bayesian Ridge RMSE: 10.9456229106651

Onehot - 38
λ python elasticnet.py
Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.795624160784076
Cross-validated Bayesian Ridge RMSE: 10.953235270148186


country : 

Best Alpha for Elastic Net: 0.01
Best l1_ratio for Elastic Net: 0.9
Cross-validated Elastic Net RMSE: 10.413730846357517
Cross-validated Bayesian Ridge RMSE: 10.582819583387598

λ python lasso_X.py
Best Alpha: 0.01
Cross-validated Lasso RMSE: 10.412954107720036

λ python ridge_X.py
Best Alpha: 1
Cross-validated Ridge RMSE: 10.417462298309491




## Best working code : 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import ElasticNet, BayesianRidge
from xgboost import XGBRegressor
import xgboost as xgb


# Load datasets 
train_data = pd.read_csv("../data/train_onehotEncoded20.csv")
train_data.dropna(inplace=True)
test_data = pd.read_csv("../data/test_full_onehotEncoded20.csv")
save_path = "predictions_stacking.csv"

# Feature selection
selected_features = [
    'Aerial Duels won', 
    'Age', 
    'Assists', 
    'Attacking options created', 
    'Attempted Passes', 
    'Blocks', 
    'Clearances', 
    'Expected Goal Contributions', 
    'Interceptions', 
    'Open Play Goals', 
    'Open Play Expected Goals', 
    'Percentage of Passes Completed', 
    'Progressive Passes Rec', 
    'Progressive Passes', 
    'Progressive Carries', 
    'Shots', 
    'Successful Dribbles', 
    'Touches in attacking penalty area', 
    'Tackles', 
    'Value at beginning of 2020/21 season', 
    'Value at beginning of 2021/22 season', 
    'Value at beginning of 2022/23 season',
    
    # 'Country_encoded'
    
    
    'Country_Other',
    
    'Country_Spain',
    'Country_France',
    'Country_Germany',

    'Country_England',
    'Country_Italy',

    'Country_Brazil',
    
    'Country_Argentina',
    
    'Country_Portugal',
    'Country_Netherlands',
    
    'Country_Denmark',
    'Country_Belgium',
    'Country_Croatia',
    
    # 'Country_Algeria',
    # 'Country_Ghana',
    # 'Country_Austria',
    # 'Country_Nigeria',
    # 'Country_Uruguay',
    # 'Country_Morocco',
    # 'Country_Senegal',
    # 'Country_Serbia',
    # 'Country_Colombia',

    # 'Country_Norway',
    # 'Country_Scotland',
    # 'Country_Switzerland',
    # 'Country_Turkey',
]

X_train = train_data[selected_features]
y_train = train_data['Value at beginning of 2023/24 season']
X_test = test_data[selected_features]

# # Transform the target variable during training
# y_train_transformed = np.log1p(y_train)

# Create a list of base models
base_models = [
    ('elasticnet', ElasticNet(alpha=0.01, l1_ratio=0.9)),
    ('Bayesian', BayesianRidge()),
    ('ridge', Ridge(alpha=1)),
    ('lasso', Lasso(alpha=0.01)),
    ('xgb', xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    max_depth=3,  # You can adjust this hyperparameter for regularization
    learning_rate=0.1,  # Adjust the learning rate as needed
    subsample=0.7,  # Adjust subsample to control overfitting
    colsample_bytree=0.8  # Adjust colsample_bytree to control overfitting
)),  # Add XGBoost as a base model
]

# Create a stacking ensemble with a meta-model
stacking_model = StackingRegressor(estimators=base_models, final_estimator=Ridge(alpha=1))

# Train the stacking ensemble and prediction

stacking_model.fit(X_train, y_train)
predictions = stacking_model.predict(X_test)

# stacking_model.fit(X_train, y_train_transformed)
# predictions_transformed = stacking_model.predict(X_test)
# predictions = np.expm1(predictions_transformed)     

# Save predictions to a new file
output_df = pd.DataFrame({'id': test_data['id'], 'label': predictions})
output_df.to_csv(save_path, index=False)
